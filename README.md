(Пояснительная записка для Хакатона: данное решение рассчитано на телеграмм, поскольку в MAX нет возможности читать предыдущие сообщения в групповом чате, но поскольку MAX молодая и развивающаяся платформа, то, возможно, что такая функциональность появиться. 
А для демонстрации решения использовался TG, где нужная функциональность есть)

# Growth Project

Сервис для автоматической суммаризации сообщений из Telegram чатов и возможности задавать вопросы по собранным данным.

## Структура проекта

```
Growth_project/
├── .env                      # Переменные окружения
├── Dockerfile.microservice   # Dockerfile для микросервиса обработки
├── Dockerfile.server        # Dockerfile для основного сервера
├── Dockerfile.tgbot         # Dockerfile для Telegram бота
├── docker-compose.yml       # Конфигурация для Docker Compose
├── requirements.txt         # Python зависимости для основного приложения
├── tg_requir.txt           # Python зависимости для Telegram бота
├── client.py               # Клиентская часть приложения
├── server.py               # Основной сервер приложения
├── microservice.py         # Микросервис для обработки данных
├── models.py               # Модели данных
├── messages_schema.yaml    # Схема для сообщений
├── topics_schema.yaml      # Схема для топиков
├── tg_sessions/            # Папка для файлов сессий Telegram
│   └── bot                 # Файл сессии бота (требуется создать)
└── LICENSE                 # Лицензия проекта
```

## Первоначальная настройка

### 1. Подготовка файлов сессии

Перед запуском создайте папку `tg_sessions` и поместите в неё файл сессии Telegram с именем `bot`:

```bash
mkdir tg_sessions
# Скопируйте ваш файл сессии в tg_sessions/bot
```

### 2. Настройка переменных окружения

Отредактируйте файл `.env` согласно вашим настройкам:

```bash
# Пример содержимого .env
TELEGRAM_API_ID=your_api_id
TELEGRAM_API_HASH=your_api_hash
DATABASE_URL=your_database_url
# ... другие переменные
```

## Запуск проекта

### 1. Сборка и запуск контейнеров

```bash
docker-compose up --build
```

### 2. Инициализация модели Ollama

**Важно:** При первом запуске необходимо выполнить следующую команду в контейнере ollama:

```bash
docker exec -it <ollama_container_name> ollama run gemma3:12b-it-qat
```

Где `<ollama_container_name>` - имя контейнера ollama (можно узнать через `docker ps`).

Эта команда загрузит и инициализирует модель Gemma 12B, которая используется для суммаризации и анализа текста.

## Технологический стек

- **FastAPI** - современный веб-фреймворк для создания API с автоматической документацией
- **Kafka** - брокер сообщений для асинхронной обработки запросов между сервисами
- **Redis** - кэширование и хранение промежуточных результатов
- **RedisVL** - векторная база данных на основе Redis для RAG-поиска
- **Ollama** - локальное выполнение LLM модели Gemma 12B для суммаризации и анализа
- **Gemma 3 12B** - языковая модель для генерации саммари и ответов на вопросы


## API Endpoints

### Основные эндпоинты (server.py):

#### `POST /api/v1/chat/process`
**Запуск обработки чата**
- Принимает список сообщений чата для обработки
- Отправляет данные в микросервис через Kafka
- Возвращает `request_id` для отслеживания прогресса

**Пример запроса:**
```json
{
  "chat_name": "Рабочий чат",
  "messages": [
    {
      "sender": "Алиса",
      "text": "Привет всем! Как дела с проектом?",
      "timestamp": "2025-01-15T10:00:00Z"
    },
    {
      "sender": "Боб",
      "text": "Все идет по плану, завтра презентация",
      "timestamp": "2025-01-15T10:01:00Z"
    }
  ]
}
```

#### `GET /api/v1/chat/process/{request_id}/stream`
**Получение потока результатов обработки**
- Возвращает Server-Sent Events с прогрессом обработки
- Показывает промежуточные результаты и финальные данные

#### `GET /api/v1/chat/process/{request_id}/status`
**Получение статуса обработки**
- Проверка текущего состояния обработки запроса
- Статусы: pending, processing, completed, error

#### `GET /api/v1/chat/requests`
**Список активных запросов**
- Показывает все текущие запросы на обработку
- Полезно для мониторинга нагрузки системы

#### `POST /api/v1/chat/query`
**RAG-поиск по обработанному чату**
- Поиск информации в обработанных сообщениях
- Использует векторный поиск + генерацию ответа через LLM

**Пример запроса:**
```json
{
  "request_id": "550e8400-e29b-41d4-a716-446655440000",
  "query": "Какие проблемы обсуждались в проекте?"
}
```

#### `GET /health`
**Проверка здоровья сервиса**
- Мониторинг состояния всех компонентов системы

## Функциональность

### Основные возможности:
- **Автоматическая суммаризация** сообщений из Telegram чатов
- **RAG-поиск** по обработанным данным с использованием векторной базы данных
- **Потоковая обработка** через Server-Sent Events для real-time обновлений
- **Асинхронная архитектура** с использованием Kafka для масштабируемости
- **API для интеграции** с автоматической документацией через FastAPI

### Компоненты системы:
- **Telegram Bot** - собирает сообщения из чатов
- **Server (FastAPI)** - основной API сервер с REST endpoints
- **Microservice** - обработка и анализ данных через Ollama
- **Kafka** - очередь сообщений для асинхронной обработки
- **RedisVL** - векторная база данных для RAG-поиска
- **Client** - клиентское приложение для взаимодействия

## Использование

После успешного запуска всех сервисов:

1. **Telegram бот** будет доступен для взаимодействия в чатах
2. **FastAPI сервер** будет доступен по адресу `http://localhost:8000`
3. **Автоматическая документация API** доступна по адресу `http://localhost:8000/docs`
4. **Клиентское приложение** можно использовать для взаимодействия с системой

### Взаимодействие через Telegram бота

Система предоставляет удобный интерфейс для работы непосредственно в Telegram чатах через специального бота.

#### Доступные команды:

**`/sum <число>`** - Запуск суммаризации последних N сообщений
- Диапазон: от 1 до 100 сообщений
- Бот автоматически исключает собственные сообщения и команды
- Обрабатывает ответы на сообщения для контекста
- Пример: `/sum 50` - обработает последние 50 сообщений

**`/ask <текст>`** - RAG-поиск по обработанным сообщениям
- Работает только после завершения суммаризации
- Использует векторный поиск + генерацию ответа через LLM
- Пример: `/ask Какие проблемы обсуждались в проекте?`

**`/status`** - Проверка статуса обработки в текущем чате
- Показывает активные процессы суммаризации
- Отображает информацию о завершенных процессах
- Индикатор доступности команды `/ask`

**`/help`** - Справка по командам и их использованию

#### Рабочий процесс через Telegram бота:

1. **Запуск суммаризации:**
   ```
   /sum 30
   ```
   Бот начнет обработку последних 30 сообщений и будет отправлять updates в реальном времени

2. **Отслеживание прогресса:**
   - Бот автоматически уведомляет о начале обработки
   - Показывает промежуточные результаты по темам
   - Уведомляет о завершении с активацией команды `/ask`

3. **Поиск информации:**
   ```
   /ask Когда планируется релиз?
   ```
   Система найдет релевантные сообщения и сгенерирует ответ
   (Примечание: ответ может содержать ошибочную информацию с малой вероятностью, поэтому нельзя его считать юридическоц информацией)

5. **Проверка статуса:**
   ```
   /status
   ```
   Получение информации о текущем состоянии обработки

#### Особенности работы бота:

- **Умная фильтрация:** Исключает системные сообщения, команды и собственные сообщения
- **Обработка ответов:** Учитывает контекст ответов на сообщения
- **Защита от дублирования:** Только одна суммаризация на чат одновременно
- **Real-time обновления:** Потоковое отображение результатов обработки
- **Персистентность:** Сохранение состояния между запросами

### Рабочий процесс через API:
1. Отправьте POST запрос на `/api/v1/chat/process` с сообщениями чата
2. Получите `request_id` и отслеживайте прогресс через `/stream` endpoint
3. После завершения обработки используйте `/api/v1/chat/query` для поиска информации
4. RAG-система найдет релевантные сообщения и сгенерирует ответ через Gemma 12B

## Требования

- Docker и Docker Compose
- Файл сессии Telegram
- Достаточно места на диске для модели Ollama (несколько ГБ)

## Лицензия

Проект распространяется под лицензией, указанной в файле LICENSE.

----

Команда Growth предоставляет сервис для пользования в состоянии как есть и не несёт какой-либо ответсвенности за проблемы повлечённые его использованием
